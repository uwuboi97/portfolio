<!DOCTYPE html>
<html>

<head>
    <meta http-equiv="content-type" content="text/html; charset=UTF-8">
    <meta charset="UTF-8">
    <meta author="Billie">
    <title>works</title>
    <meta name="viewport" content="initial-scale=1">
       <link rel="stylesheet" href="../style.css">


 
</head>

<body>


    <div id="text">
        <div class="title"><span class="name">../<a href="../index.html">billie sng</a><a href="../about.html">/about</a> </span></div>
        <div class="list">




            <ul>
                <br>
                <ul id="cat1" class="everything-list">
                    <li class="works">
                    <li>../<a href="../index.html">2021</a><a class="current">/SENSUS</a></li><br>
                    <li class="work">printed text on paper<br></li>
                    <li class="body">Latin: sensus, sens-us m.
                        English: feeling/perception/sense/emotion
                        
                         
                        
                        SENSUS is a continuation of my exploration of the ontology of language and text and its direct relations to the limits of man which began last year. This is an attempt to resolve my sensus of text, id est the emotions, sensations, and feelings one perceives when seeing or reading text. At its core, it is a reinvention of the cut-up technique. Using a Deep Learning recurrent neural network, it creates a dialectic between neurons and transistors, conscious and subconscious, sense and non-sense. And at the end a synthesis: my sensus of text and language made lucid in code hypotheses and writings. This research, however, is an exploration of methodology more than anything else and is still in its infancy.<br><br>

                        The original cut-up technique was popularised by Beat Generation writer William Burroughs. Cutting up texts from different sources, it forces the miracle of fortuity to birth something radically new. Originally, I used pre-made neural network services to generate text, inserting short prompts that the AI continues or completes. I then collect the clippings and form new writings. One can, however, easily feel the apparent lack of control and absence of learning in this method. Just a man and his tool than a collaboration. I was thus tragically forced to learn code.<br><br>
                        Deep Learning creates an artificial neural network (synapses) which, as its name suggests, learns with or without supervision. With the GPT-2 a language model by OpenAI, I inject my archive of found texts, short phrases, or entire essays to train the neural network which can be tuned to a higher degree but more importantly evolves. The results can range from coherent short stories to complete gibberish. It is ultimately up to me to choose what gets made; the concept creates the medium.
                        <br><br>
                        At the present moment, two journal entries have been made with the initial, more rudimentary method, and a few stories with the more advanced neural network. A commonality between both methods is this conscious act of curating or “weaving” texts. The word “text” comes from the Latin texere, which means “to weave.” I would like to think of his work like that. It is a simple organisation to form a proposition. The organisation of language. Every writing is a confession of the soul, how one expresses it is a matter of personal poetics. And it is that organisation that has meaning to me.

<br><br>

                            <img src="iwantedtodiehere_files/SENSUS_1.jpg" alt="1" style="width:80%">
                            <img src="iwantedtodiehere_files/SENSUS_2.jpg" alt="1" style="width:80%">
                            <img src="iwantedtodiehere_files/SENSUS_3.jpg" alt="1" style="width:80%">

                      
                       

                    




            <br><br>
        </div>
    </div>



</body></html>
